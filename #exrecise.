
# coding: utf-8

# In[1]:
#kNN

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# In[2]:


import seaborn as sns
get_ipython().run_line_magic('matplotlib', 'inline')


# In[7]:


df=pd.read_csv('DATA.csv')


# In[8]:


df.head()


# In[9]:


df.info()


# In[10]:


df.describe()


# In[11]:


from sklearn.preprocessing import StandardScaler


# In[12]:


scaler=StandardScaler()


# In[21]:


scaler.fit(df.drop('TARGET CLASS', axis=1))


# In[22]:


scaled_features=scaler.transform(df.drop('TARGET CLASS',axis=1))


# In[27]:


df_feat=pd.DataFrame(scaled_features,columns=df.columns[:-1])


# In[28]:


from sklearn.cross_validation import train_test_split


# In[29]:


df_feat.columns


# In[32]:


X=df_feat
y=df['TARGET CLASS']


# In[34]:


X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=101)


# In[35]:


from sklearn.neighbors import KNeighborsClassifier


# In[36]:


knn=KNeighborsClassifier(n_neighbors=1)


# In[37]:


knn.fit(X_train, y_train)


# In[38]:


p=knn.predict(X_test)


# In[40]:


from sklearn.metrics import classification_report,confusion_matrix


# In[41]:


print (confusion_matrix(y_test,p))
print (classification_report(y_test,p))


# In[55]:


error_rate=[]
for i in range(1,40):
    knn=KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    p_i=knn.predict(X_test)
    error_rate.append(np.mean(p_i!=y_test))


# In[53]:


plt.figure(figsize=(10,6))
plt.plot(error_rate,color='red',marker='o',markersize=10,)


# In[62]:


knn=KNeighborsClassifier(n_neighbors=18)
knn.fit(X_train,y_train)
p=knn.predict(X_test)

print (confusion_matrix(y_test,p))
print ('\n')
print (classification_report(y_test,p))

